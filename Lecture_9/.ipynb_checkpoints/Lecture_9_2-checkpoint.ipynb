{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Lecture 8.2 Boosting \n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/RandyRDavila/Data_Science_and_Machine_Learning_Spring_2022/blob/main/Lecture_9/Lecture_9_2.ipynb)\n",
    "\n",
    "**Boosting** is an ensemble method that trains predictors sequentially, each trying to correct the errors made by the previous predictor. In this lecture we consider two well known boosting methods, namely **AdaBoost** and **gradient boosting**. \n",
    "\n",
    "## AdaBoost\n",
    "With **AdaBoost**, the training algorithm first trains a base classifier and uses it to make predictions on the training set. Then, each of the missclassified training instances is then given a *relative weight*. The next classifier is then trained on the dataset using these relative weights, and so on. \n",
    "\n",
    "The idea is that whenever a classifier missclassifies a data point,this data point is then *boosted* to signal difficulty in classification. \n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Set theme for plotting\n",
    "sns.set_theme()\n",
    "\n",
    "# Import the data\n",
    "iris = sns.load_dataset(\"iris\")\n",
    "iris = iris.iloc[50:]\n",
    "\n",
    "X = iris[[\"sepal_length\", \"sepal_width\"]].to_numpy()\n",
    "\n",
    "# Define labeling function\n",
    "def make_labels(y):\n",
    "    if y == \"versicolor\":\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "        \n",
    "# Create target value array\n",
    "y = iris[\"species\"].map(make_labels).to_numpy()\n",
    "\n",
    "# Plot the data\n",
    "flowers = [\"versicolor\", \"virginica\"]\n",
    "colors = [\"magenta\", \"lightseagreen\"]\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (10, 8))\n",
    "for species, color in zip(flowers, colors):\n",
    "    temp_df = iris[iris.species == species]\n",
    "    ax.scatter(temp_df.sepal_length,\n",
    "               temp_df.sepal_width,\n",
    "               c = color,\n",
    "               label = species, \n",
    "               )\n",
    "    \n",
    "ax.set_xlabel(\"sepal length [cm]\", fontsize = 15)\n",
    "ax.set_ylabel(\"sepal width [cm]\", fontsize = 15)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y,\n",
    "                                                    test_size = 0.4, \n",
    "                                                    random_state = 42)\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1, random_state=42), \n",
    "                             n_estimators = 10,\n",
    "                             algorithm = \"SAMME.R\",\n",
    "                             learning_rate = 0.5)\n",
    "ada_clf.fit(X_train, y_train)\n",
    "ada_y_pred = ada_clf.predict(X_test)\n",
    "print(f\"AdaBoost Classification Report\")\n",
    "print(classification_report(y_test, ada_y_pred), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "As you will notice by running the following code cell, the decision regions generated by AdaBoost are distinctly different from those generated by the bagging and random forest algorithms. \n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "plt.figure(figsize = (10, 8))\n",
    "plot_decision_regions(X, y, clf = ada_clf)\n",
    "plt.xlabel(\"feature x_0\", fontsize = 15)\n",
    "plt.ylabel(\"feature x_1\", fontsize = 15)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris[[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]].to_numpy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y,\n",
    "                                                    test_size = 0.4, \n",
    "                                                    random_state = 42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y,\n",
    "                                                    test_size = 0.4, \n",
    "                                                    random_state = 42)\n",
    "\n",
    "ada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1, random_state=42), \n",
    "                             n_estimators = 10,\n",
    "                             algorithm = \"SAMME.R\",\n",
    "                             learning_rate = 0.5)\n",
    "ada_clf.fit(X_train, y_train)\n",
    "ada_y_pred = ada_clf.predict(X_test)\n",
    "print(f\"Tree Classification Report\")\n",
    "print(classification_report(y_test, ada_y_pred), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Gradient Boosting \n",
    "Another popular boosting method is **gradient boosting**. This method works by sequentially adding predictors to an ensemble, each correcting is predecessor. The difference between this method and AdaBoost is that gradient boosting tries to fit the new predictor to the *residual errors* made by the previous predictor. Recall the residual error denoted by $error$. \n",
    "\n",
    "Given features ```X``` and regression values ```y```, we train a regression tree ```tree_reg1``` on this dat and then ```tree_reg1``` predicts a vector $\\hat{y}_i$ of real values. Then, \n",
    "\n",
    "$$\n",
    "y = \\hat{y}_1 + error_1\n",
    "$$\n",
    "\n",
    "which implies, \n",
    "$$\n",
    "y - \\hat{y}_1 = error_1\n",
    "$$\n",
    "\n",
    "Next fit another regression tree, say ```tree_reg2```, with features ```X``` and labels $y_2 = y - \\hat{y}1$. Then, ```tree_reg2``` predict a vector $\\hat{y}_2$, with \n",
    "\n",
    "$$\n",
    "error_1 = y_2 = \\hat{y}_2 + error_2\n",
    "$$\n",
    "\n",
    "Thus, \n",
    "\n",
    "$$\n",
    "y = \\hat{y}_1 + error_1 = \\hat{y}_1 + \\hat{y}_2 + error_2, \n",
    "$$\n",
    "where $error_1 > error_2$. This means that we have a better predictor by summing the predictions of ```tree_reg1``` and ```tree_reg2```!\n",
    "\n",
    "\n",
    "In the following code cells we generate artifical quadratic data and then perform gradient boost with three regression trees. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "X = np.linspace(-.9, .9, 100)\n",
    "y = (X ** 4) + np.random.normal(-.09, .09, 100)\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(X, y, s=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "X_new = X.reshape(-1, 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_new, \n",
    "                                                    y,\n",
    "                                                    test_size = 0.4, \n",
    "                                                    random_state = 42)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tree_reg1 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
    "tree_reg1.fit(X_train, y_train)\n",
    "\n",
    "y2 = y_train - tree_reg1.predict(X_train)\n",
    "\n",
    "tree_reg2 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
    "tree_reg2.fit(X_train, y2)\n",
    "\n",
    "y3 = y2 - tree_reg2.predict(X_train)\n",
    "\n",
    "tree_reg3 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
    "tree_reg3.fit(X_train, y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15,10))\n",
    "\n",
    "ax1.scatter(X, y, s=10)\n",
    "ax1.plot(X, tree_reg1.predict(X_new), color = \"red\")\n",
    "ax1.set_title(\"h(x) = h_1(x)\", fontsize = 18)\n",
    "\n",
    "ax2.scatter(X, y, s=10)\n",
    "ax2.plot(X, tree_reg1.predict(X_new)+tree_reg2.predict(X_new), color = \"red\")\n",
    "ax2.set_title(\"h(x) = h_1(x) + h_2(x)\", fontsize = 18)\n",
    "\n",
    "ax3.scatter(X, y, s=10)\n",
    "ax3.plot(X, tree_reg1.predict(X_new)+tree_reg2.predict(X_new)+tree_reg3.predict(X_new), color = \"red\")\n",
    "ax3.set_title(\"h(x) = h_1(x) + h_2(x) + h_3(x)\", fontsize = 18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
